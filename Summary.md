## __Summary__ </br>

<p>    Over the past 5 years competitive gaming viewership has surged by about 8%. Sports have long used exploratory analytics and prediction analytics methods to help inform training techniques and conditioning. This same idea can be applied to professional gaming. Using Extreme Gradient Boosting (XGBoost) as a method to fit a model check our accuracy and visualize the most predictive features and how they affect outcome (win/loss). XGBoost uses a form of boosting as an ensemble method along with its loss function to minimize the Mean Squared Error (MSE). Using gradient descent XGBoost will update our initial predictions based on a learning rate, we can then find the values where MSE is minimum. To get an idea of what type of tree XGBoost will create we first train a cross-validated decision tree model. This produced an accuracy metric (AUC) of 79%. Using XGBoost we produce 99.99% (AUC). We can also see in our actual vs predicted plot a visual representation of our model fit. For visualization purposes we will refit our XGBoost with only the top 10 most predictive features. The reduced model was fit with the same accuracy as our first shot. Not unsurprisingly the top 3 most predictive features where kills, assists, and deaths. Some noticeable trends with these three features; as each feature is less predictive than the last, we see each graph move from homoscedasticity towards a more heteroscedastic pattern. Kills has a large variance between 2-4 range which could be explored further. Finally, the output shows that the higher any feature reaches in value, the less predictive each feature becomes. This could be indicating that the length of the game (time) plays a role in predictiveness for each feature. </p>
